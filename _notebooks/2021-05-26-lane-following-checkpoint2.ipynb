{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Part 1: Lane Following using Comma AI dataset\n",
    "\n",
    "This is the first post in the series of posts on end-to-end model for autonomous driving. Future posts:\n",
    "- Part 2: Using Vision Transformers with Comma AI dataset\n",
    "- Part 3: Using PilotNet model in real life on Estonian gravel roads"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Introduction\n",
    "\n",
    "- Imitation learning\n",
    "- NHTSA Level\n",
    "\n",
    "![EndToEnd](images/end-to-end.learning.jpg \"Credit: https://twitter.com/haltakov/status/1384192583597912065\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data\n",
    "\n",
    "[Comma AI dataset](https://github.com/commaai/comma2k19) is used to train the model. This dataset has over 33 hours of\n",
    "commute in California's highway. Dataset is divided into 95% training, 5% validation and 5% test set.\n",
    "\n",
    "Comma AI dataset contains a small sample of very difficult situations like crossroads, which are impossible to predict\n",
    "correctly using just camera images as model has no clear information whether to turn left or right. Most of these cases\n",
    "have high steering angle and make it very hard for model to converge (especially with using MSE loss). To avoid manually\n",
    "going through hours of videos, all frames with steering angle bigger than 20 degrees are removed from dataset.\n",
    "(TODO: include exact counts of frames removed)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model\n",
    "Convolutional neural network have been most succesful architectures in computer vision and it is natural choice for lane\n",
    "following. NVIDIA used CNN architecture in their DAVE-2 system called PilotNet 1.\n",
    "\n",
    "![PilotNet](images/pilotnet-architecture.png \"PilotNet architecture defined in Nvidia paper\")\n",
    "\n",
    "I used Batch normalisation instead of first static normalisation layer as I found it made training more stable,\n",
    "model trained quicker and had less variability in epochs validation losses. Also Leaky ReLU is used as activation\n",
    "function for layers.\n",
    "\n",
    "|Layer name|Output size|Number of parameters|\n",
    "|---|---|---|\n",
    "|Convolution2D 1|   |   |\n",
    "|BatchNorm2D 1|   |   |\n",
    "|LeakyRelu 1|   |   |\n",
    "|Convolution2D 2|   |   |\n",
    "|BatchNorm2D 2|   |   |\n",
    "|LeakyRelu 2|   |   |\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training\n",
    "\n",
    "Model is trained until validation loss fails to improve for 10 epochs. Mean absolute error (MAE) is used  as loss function\n",
    "as it proved to work better compared to mean square error (MSE) loss as it does not magnify errors with big steering angles.\n",
    "\n",
    "Big effort was needed to speed up training speed as training on video files is very slow, this is magnified with the need\n",
    "to access frames randomly during training. I tried to speed up the processing by using [NVIDIA DALI](https://docs.nvidia.com/deeplearning/dali/user-guide/docs),\n",
    "but reading video frames was still bottleneck and not GPU. The best performance was achieved by extracting all frame from video\n",
    "files into JPG files with reduced resolution using *ffmpeg* utility and training model using these.\n",
    "\n",
    "TODO: hyperparameter tuning"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Results\n",
    "\n",
    "After training 8 epochs, although training loss was still going lower, validation loss stopped improving, so training\n",
    "was stopped with following metrics:\n",
    "\n",
    "Training loss: **0.9257**<br/>\n",
    "Validation loss: **1.258**\n",
    "\n",
    "(TODO: include graphs from W&B)\n",
    "\n",
    "Model performance using videos from validation dataset (green is true steering angle and red is predicted steering angle):\n",
    "> youtube: https://youtu.be/iR3qkDQD_Pk\n",
    "> youtube: https://youtu.be/p1wjzHW8HCY\n",
    "\n",
    "#### Data balancing\n",
    "\n",
    "Driving data is very unbalanche, most driving is done straigh or with very small steering angle. This can be seen also\n",
    "in Comma AI dataset:\n",
    "\n",
    "TODO: pic with steering angle\n",
    "\n",
    "This presents problem for training neural network as it will be biased to predict small degrees and underpredict bigger\n",
    "steering angles. I test with two balanced datasets, but these did not improve results and something more clever needs to\n",
    "be done to remove the bias.\n",
    "\n",
    "TODO: pics of balanced steering angles.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Visualisation of network state\n",
    "[Grad-CAM](https://arxiv.org/abs/1610.02391) paper introduces technique for producing \"visual explanations\" for decisions\n",
    "from a large class of CNN-based models. There is great [pytorch implementation](https://github.com/jacobgil/pytorch-grad-cam)\n",
    "implemention for class prediction, which I modified to work with regression problem to display activation maps.\n",
    "\n",
    "First layer seems to provides best information. Model seems to be mostly concentrating on road markings, ground under\n",
    "other cars and sides of the road:\n",
    "> youtube: https://youtu.be/hlQyDc7xGMc\n",
    "\n",
    "## Learnings\n",
    "- Most effort will go preparing data pipeline and not tuning model itself\n",
    "- Gradient based visualisation can provide insights into how model works\n",
    "\n",
    "## Conclusions\n",
    "\n",
    "## References\n",
    "\n",
    "1 Pilotnet https://arxiv.org/abs/1604.07316"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}