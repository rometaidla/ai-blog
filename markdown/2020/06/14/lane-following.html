<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Lane Following using Comma-AI dataset | Just another AI blog</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Lane Following using Comma-AI dataset" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Lane Following using Comma-AI dataset." />
<meta property="og:description" content="Lane Following using Comma-AI dataset." />
<link rel="canonical" href="https://rometaidla.github.io/ai-blog/markdown/2020/06/14/lane-following.html" />
<meta property="og:url" content="https://rometaidla.github.io/ai-blog/markdown/2020/06/14/lane-following.html" />
<meta property="og:site_name" content="Just another AI blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-06-14T00:00:00-05:00" />
<script type="application/ld+json">
{"description":"Lane Following using Comma-AI dataset.","url":"https://rometaidla.github.io/ai-blog/markdown/2020/06/14/lane-following.html","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://rometaidla.github.io/ai-blog/markdown/2020/06/14/lane-following.html"},"headline":"Lane Following using Comma-AI dataset","dateModified":"2020-06-14T00:00:00-05:00","datePublished":"2020-06-14T00:00:00-05:00","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/ai-blog/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://rometaidla.github.io/ai-blog/feed.xml" title="Just another AI blog" /><link rel="shortcut icon" type="image/x-icon" href="/ai-blog/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />

<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/ai-blog/">Just another AI blog</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/ai-blog/about/">About Me</a><a class="page-link" href="/ai-blog/search/">Search</a><a class="page-link" href="/ai-blog/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Lane Following using Comma-AI dataset</h1><p class="page-description">Lane Following using Comma-AI dataset.</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2020-06-14T00:00:00-05:00" itemprop="datePublished">
        Jun 14, 2020
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      5 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/ai-blog/categories/#markdown">markdown</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h1"><a href="#part-1-lane-following-using-comma-ai-dataset">Part 1: Lane Following using Comma AI dataset</a>
<ul>
<li class="toc-entry toc-h2"><a href="#introduction">Introduction</a></li>
<li class="toc-entry toc-h2"><a href="#data">Data</a>
<ul>
<li class="toc-entry toc-h3"><a href="#model">Model</a></li>
<li class="toc-entry toc-h3"><a href="#training">Training</a>
<ul>
<li class="toc-entry toc-h4"><a href="#data-balancing">Data balancing</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#results">Results</a></li>
<li class="toc-entry toc-h2"><a href="#visualisation-of-network-state">Visualisation of network state</a></li>
<li class="toc-entry toc-h2"><a href="#conclusions">Conclusions</a>
<ul>
<li class="toc-entry toc-h3"><a href="#further-improvements">Further improvements</a></li>
<li class="toc-entry toc-h3"><a href="#learnings">Learnings</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#references">References</a></li>
</ul>
</li>
</ul><h1 id="part-1-lane-following-using-comma-ai-dataset">
<a class="anchor" href="#part-1-lane-following-using-comma-ai-dataset" aria-hidden="true"><span class="octicon octicon-link"></span></a>Part 1: Lane Following using Comma AI dataset</h1>

<p>This is the first post in the series of posts on end-to-end model for autonomous driving. Future posts:</p>
<ul>
  <li>Part 2: Using Vision Transformers with Comma AI dataset</li>
  <li>Part 3: Using PilotNet model in real life on Estonian gravel roads</li>
</ul>

<h2 id="introduction">
<a class="anchor" href="#introduction" aria-hidden="true"><span class="octicon octicon-link"></span></a>Introduction</h2>

<ul>
  <li>Imitation learning</li>
  <li>NHTSA Level</li>
</ul>

<p><img src="/ai-blog/images/lanefollowing/end-to-end-learning.jpg" alt="EndToEnd" title="Credit: https://twitter.com/haltakov/status/1384192583597912065"></p>

<h2 id="data">
<a class="anchor" href="#data" aria-hidden="true"><span class="octicon octicon-link"></span></a>Data</h2>

<p><a href="https://github.com/commaai/comma2k19">Comma AI dataset</a> is used to train the model. This dataset has over 33 hours of
commute in California’s highway. Dataset is divided into 95% training, 5% validation and 5% test set.</p>

<p>Video resolution is 1164x874. When extracting frames for training, image is downscaled to the resolution of 258x194 for
faster training process. From this downscaled image, smaller region of interest is cropped as most of the image does not
in include information useful for training, like trees and sky.</p>

<p><img src="/ai-blog/images/lanefollowing/roi.png" alt="RegionOfInterest" title="Region of interest used for training is marked with red box."></p>

<p>Comma AI dataset contains a small sample of very difficult situations like crossroads, which are impossible to predict
correctly using just camera images as model has no clear information whether to turn left or right. Most of these cases
have high steering angle and make it very hard for model to converge (especially with using MSE loss). To avoid manually
going through hours of videos, all frames with steering angle bigger than 20 degrees are removed from dataset.
(TODO: include exact counts of frames removed)</p>

<h3 id="model">
<a class="anchor" href="#model" aria-hidden="true"><span class="octicon octicon-link"></span></a>Model</h3>
<p>Convolutional neural network have been most succesful architectures in computer vision and it is natural choice for lane
following. NVIDIA used CNN architecture in their DAVE-2 system called PilotNet [1].</p>

<p><img src="/ai-blog/images/lanefollowing/pilotnet-architecture.png" alt="PilotNet" title="PilotNet architecture defined in Nvidia paper"></p>

<p>I used Batch normalisation instead of first static normalisation layer as I found it made training more stable,
model trained quicker and had less variability in epochs validation losses. Also Leaky ReLU is used as activation
function for layers.</p>

<h3 id="training">
<a class="anchor" href="#training" aria-hidden="true"><span class="octicon octicon-link"></span></a>Training</h3>

<p>Model is trained until validation loss fails to improve for 10 epochs. Mean absolute error (MAE) is used  as loss function
as it proved to work better compared to mean square error (MSE) loss as it does not magnify errors with big steering angles.</p>

<p>Big effort was needed to speed up training speed as training on video files is very slow, this is magnified with the need
to access frames randomly during training. I tried to speed up the processing by using <a href="https://docs.nvidia.com/deeplearning/dali/user-guide/docs">NVIDIA DALI</a>,
but reading video frames was still bottleneck and not GPU. The best performance was achieved by extracting all frame from video
files into JPG files with reduced resolution using <em>ffmpeg</em> utility and training model using these.</p>

<p>TODO: hyperparameter tuning</p>

<h4 id="data-balancing">
<a class="anchor" href="#data-balancing" aria-hidden="true"><span class="octicon octicon-link"></span></a>Data balancing</h4>

<p>Driving data is very unbalanced, most driving is done straight or with very small steering angle. This can be seen also
in Comma AI dataset:</p>

<p><img src="/ai-blog/images/lanefollowing/unbalanced-data.png" alt="Unbalanced" title="Comma AI dataset is very unbalances, most steering angles are near 0 degrees."></p>

<p>This presents problem for training neural network as it will be biased to predict small degrees and under-predict bigger
steering angles. I tested with two balanced datasets</p>

<table>
  <tbody>
    <tr>
      <td><img src="/ai-blog/images/lanefollowing/balanced-data.png" alt="Balanced" title="Balanced dataset by oversampling images with less frequent steering angles so that we get uniform distribution."></td>
      <td><img src="/ai-blog/images/lanefollowing/fattails-data.png" alt="Fattails" title="Balanced dataset by oversampling image with less frequent steering angles so that tails of distribution are fatter."></td>
    </tr>
  </tbody>
</table>

<h2 id="results">
<a class="anchor" href="#results" aria-hidden="true"><span class="octicon octicon-link"></span></a>Results</h2>

<p>Training loss improved mostly during first 5 iteration for every data balancing variation as this dataset is quite big 
and has similar driving data.</p>

<p><img src="/ai-blog/images/lanefollowing/trainloss.png" alt="Train loss" title="Training loss"></p>

<p>Similarly validation loss drops quickly and only has minor improvement afterwards. Unbalanced dataset achieves the lowest 
validation loss already on 7th epoch and fat-tailed balanced dataset on 11th epoch. Unbalanced and fat-tails data distributions
seem to train better and achieve lower validation loss compared to uniformly balanced dataset.</p>

<p><img src="/ai-blog/images/lanefollowing/valloss.png" alt="Validation loss" title="Validation loss"></p>

<p>Model trained with fat-tailed distribution got the lowest test loss of <strong>1.107</strong>, followed by model with unbalance
data <strong>1.157</strong>. Model trained with uniform distribution got the highest test loss of <strong>1.223</strong>, which is probably caused
by changing initial distribution too much and causing model to overestimate small steering angles.</p>

<p>Oversampling data to fat-tailed distribution seems to be promising and training
model with even fatter tails could improve results even further. Losses were measured using only one training run and
to get more valid comparison, several runs should be made to see the variability in results.</p>

<p><img src="/ai-blog/images/lanefollowing/testloss.png" alt="Test loss" title="Test loss"></p>

<p>Model performance during the day (green is true steering angle and red is predicted steering angle):</p>

<figure class="video_container">
  <iframe width="840" height="550" src="https://www.youtube.com/embed/0ZZtgRv3__c?controls=0" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
</figure>

<p><img src="/ai-blog/images/lanefollowing/test-steering-angles-day.png" alt="Predicted steering angles during the day" title="Predicted steering angles during the day"></p>

<p>During the night:</p>
<figure class="video_container">
  <iframe width="840" height="550" src="https://www.youtube.com/embed/VQ7BT5ZxkIc?start=0&amp;controls=0" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
</figure>

<p><img src="/ai-blog/images/lanefollowing/test-steering-angles-night.png" alt="Predicted steering angles during the night" title="Predicted steering angles during during the night"></p>

<h2 id="visualisation-of-network-state">
<a class="anchor" href="#visualisation-of-network-state" aria-hidden="true"><span class="octicon octicon-link"></span></a>Visualisation of network state</h2>
<p><a href="https://arxiv.org/abs/1610.02391">Grad-CAM</a> paper [2] introduces technique for producing “visual explanations” for decisions
from a large class of CNN-based models. It is technique for visualising importance of image feature to the final output
of the network. There is great <a href="https://github.com/jacobgil/pytorch-grad-cam">pytorch implementation</a> of these gradient based methods.</p>

<p>As this technique is for classification problem and predicting steering angle is regression problem, I modified the implementation 
to work with regression problem by using ideas from Jacob Gildenblat blog [4]. Target steering angles are divided into 3 ranges,
turning strong to the left (big positive steering angles), turning strongly the right (big negative steering angles) and driving straight
(small steering angles).This makes it classification problem again. When steering angle are big, image features contributing 
most to big steering angles are peaked. When steering angle is small, image features contributing mostly to small steering 
angle by taking inverse of steering angle as our target.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">grad_cam_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">angle</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">angle</span> <span class="o">&gt;</span> <span class="mf">2.0</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">x</span>
    <span class="k">elif</span> <span class="n">angle</span> <span class="o">&lt;</span> <span class="o">-</span><span class="mf">2.0</span><span class="p">:</span>
        <span class="k">return</span> <span class="o">-</span><span class="n">x</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="n">reciprocal</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">cpu</span><span class="p">())</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">sign</span><span class="p">(</span><span class="n">angle</span><span class="p">.</span><span class="n">cpu</span><span class="p">())</span>
</code></pre></div></div>

<p>By resulting activation maps for each convolutional layer are following:</p>

<p><img src="/ai-blog/images/lanefollowing/gradcam_layers.png" alt="Gradcam layers" title="Gradcam activation maps"></p>

<p>First layer seems to provide the best information. Model seems to be mostly concentrating on road markings, ground under
other cars and sides of the road:</p>
<figure class="video_container">
  <iframe width="840" height="550" src="https://www.youtube.com/embed/hlQyDc7xGMc?controls=1" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
</figure>

<h2 id="conclusions">
<a class="anchor" href="#conclusions" aria-hidden="true"><span class="octicon octicon-link"></span></a>Conclusions</h2>

<h3 id="further-improvements">
<a class="anchor" href="#further-improvements" aria-hidden="true"><span class="octicon octicon-link"></span></a>Further improvements</h3>
<ul>
  <li>Use more complex model</li>
  <li>Use PilotNet with bigger input size</li>
  <li>Do data augmentation</li>
</ul>

<h3 id="learnings">
<a class="anchor" href="#learnings" aria-hidden="true"><span class="octicon octicon-link"></span></a>Learnings</h3>
<ul>
  <li>Most effort will go preparing data pipeline and not tuning model itself</li>
  <li>Gradient based visualisation can provide insights into how model works</li>
</ul>

<h2 id="references">
<a class="anchor" href="#references" aria-hidden="true"><span class="octicon octicon-link"></span></a>References</h2>

<p>[1] End to End Learning for Self-Driving Cars <a href="https://arxiv.org/abs/1604.07316">https://arxiv.org/abs/1604.07316</a></p>

<p>[2] Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization <a href="https://arxiv.org/abs/1610.02391">https://arxiv.org/abs/1610.02391</a></p>

<p>[3] Class Activation Map methods implemented in Pytorch <a href="https://github.com/jacobgil/pytorch-grad-cam">https://github.com/jacobgil/pytorch-grad-cam</a></p>

<p>[4] Visualizations for regressing wheel steering angles in self driving cars, Jacob Gildenblat <a href="https://jacobgil.github.io/deeplearning/vehicle-steering-angle-visualizations">https://jacobgil.github.io/deeplearning/vehicle-steering-angle-visualizations</a></p>

  </div><a class="u-url" href="/ai-blog/markdown/2020/06/14/lane-following.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/ai-blog/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/ai-blog/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/ai-blog/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>Just another blog about AI, mainly related to self-driving cars.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"></ul>
</div>

  </div>

</footer>
</body>

</html>
